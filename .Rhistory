.53*mean(c(18,24)) + .261*mean(c(25,39)) + .523*mean(c(40,59)) + .6*60
mean(c(18,24))
mean(c(25,39))
mean(c(40,59))
(.53*mean(c(18,24)) + .261*mean(c(25,39)) + .523*mean(c(40,59)) + .6*60)/(.53+.261+.523+.6)
(.053*mean(c(18,24)) + .261*mean(c(25,39)) + .523*mean(c(40,59)) + .6*60)/(.053+.261+.523+.6)
(.053*mean(c(18,24)) + .261*mean(c(25,39)) + .523*mean(c(40,59)) + .163*60)/(.053+.261+.523+.163)
# mean age from table with categories
(.053*mean(c(18,24)) + .261*mean(c(25,39)) + .523*mean(c(40,59)) + .163*60)/(.053+.261+.523+.163)
citation("naniar")
data.dir = "~/Dropbox/Personal computer/Independent studies/2019/AWR (animal welfare review meat consumption)/Linked to OSF (AWR)/Data extraction"
library(dplyr)
library(tidyverse)
library(naniar)
library(tidyr)
library(readxl)
library(irr)
library(testthat)
############################### PREP INDIVIDUAL CODERS' SUBJECTIVE RISKS OF BIAS ###############################
setwd(data.dir)
setwd("Dual review of intervention components")
list.files()
# read it back in
setwd(data.dir)
d = read.csv("data_prepped_step2.csv")
expect_equal( nrow(d), 108 )  # includes SSWS
# how many unique studies?
length(unique(d$authoryear))
# how many point estimates?
nrow(d)
# qualitative data entered into Excel
setwd(data.dir)
# NOTE: this step breaks if cell values are hyphenated!
d2 = read_xlsx("Extracted qualitative data.xlsx", na = "NR")
# remove missing rows, used for human-readability
d2 = d2 %>% filter(!is.na(`First author last name`))
# for some reason, reads in years in an absurd format (e.g., "2018.0" as a string)
library(tidyverse)
d2$Year = str_remove(d2$Year, "[.]0")
# clean up carriage returns
d2$`Subject country` = str_replace_all(d2$`Subject country`, "[\n]" , "")
##### unique merger variable
d$unique = NA
d$unique[ is.na(d$substudy) ] = as.character(d$authoryear[ is.na(d$substudy) ])
d$unique[ !is.na(d$substudy) ] = paste( d$authoryear[ !is.na(d$substudy) ],
d$substudy[ !is.na(d$substudy) ],
sep = " ")
d$unique
# make pretty for eventual forest plot
d2$unique = NA
d2$unique[ is.na(d2$`Substudy #`) ] = paste( d2$`First author last name`[ is.na(d2$`Substudy #`) ],
d2$Year[ is.na(d2$`Substudy #`) ],
sep = " ")
d2$unique[ !is.na(d2$`Substudy #`) ] = paste( d2$`First author last name`[ !is.na(d2$`Substudy #`) ],
d2$Year[ !is.na(d2$`Substudy #`) ],
d2$`Substudy #`[ !is.na(d2$`Substudy #`) ],
sep = " ")
d2$unique
# look for IDs from effect sizes that aren't in the qualitative data spreadsheet
#  (should be none)
expect_equal( d$unique[ !d$unique %in% d2$unique ], character(0) )
# studies in qualitative data for which we haven't entered effect sizes
# should occur only for hopeless studies (i.e., impossible to get stats)
known.hopeless = c( "Dowsett 2018",
#"Diaz 2019",  # removed from spreadsheet because we couldn't determine its eligibility
"de Lanauze 2019",
"Summer Vegan Pledge (Animal Aid) 2018",
"Summer Vegan Pledge (Animal Aid) 2019",
"Veganuary (Veganuary) 2017",
"Veganuary (Veganuary) 2018",
"Veganuary (Veganuary) 2019" )
expect_equal( sort(d2$unique[ !d2$unique %in% d$unique ]), sort(known.hopeless) )
# merge them
d = merge( d,
d2,
all.x = TRUE,
by.x = "unique",
by.y = "unique" )
############################### MERGE IN SUBJECTIVELY-RATED QUALITY DATA ###############################
# MM audited 2020-2-5
# read in subjective quality data
setwd(data.dir)
setwd("Dual review of quality")
d3 = read.csv("subjective_data_full_prepped.csv")
# check for studies lacking entries in subjective data
# should be only high-bias challenges
d$authoryear[ !d$authoryear %in% d3$authoryear ]
# merge with main dataset
d = merge( d,
d3[ , c("authoryear", "qual.exch", "qual.sdb", "qual.gen") ],
all.x = TRUE,
by = "authoryear" )
#setwd(data.dir)
setwd("~/Dropbox/Personal computer/Independent studies/2019/AWR (animal welfare review meat consumption)/Linked to OSF (AWR)/Data extraction/Dual review of intervention components")
list.files()
d4 = read_xl("*component_coding_reconciled.xlsx")
d4 = read_xlsx("*component_coding_reconciled.xlsx")
head(d4)
?rename
d4 = rename(d4, x.mind.attr = "mind.attribution")
names(d4)
d4 = rename(d4, x.mind.attr = "mind.attribution",
x.soc.norm = "social.norms",
x.id.victim = "id.victim",
x.impl = "impl.suggest",
x.pets = "pets")
names(d4)
setwd("~/Dropbox/Personal computer/Independent studies/2019/AWR (animal welfare review meat consumption)/Linked to OSF (AWR)/Data extraction/Dual review of intervention components")
d4 = read_xlsx("*component_coding_reconciled.xlsx")
names(d4)[ names(d4) %in% c("mind.attribution", "social.norms", "id.victim", "impl.suggest", "pets") ] = c("x.mind.attr", "x.soc.norm", "x.id.victim", "x.pets", "x.impl")
d4 = rename(d4, x.mind.attr = "mind.attribution",
x.soc.norm = "social.norms",
x.id.victim = "id.victim",
x.impl = "impl.suggest",
x.pets = "pets")
setwd("~/Dropbox/Personal computer/Independent studies/2019/AWR (animal welfare review meat consumption)/Linked to OSF (AWR)/Data extraction/Dual review of intervention components")
d4 = read_xlsx("*component_coding_reconciled.xlsx")
d4 = rename(d4, x.mind.attr = "mind.attribution",
x.soc.norm = "social.norms",
x.id.victim = "id.victim",
x.impl = "impl.suggest",
x.pets = "pets")
names(d4)
# check for studies lacking entries in subjective data
# should be only high-bias challenges
d$authoryear[ !d$authoryear %in% d4$authoryear ]
# recode "Unclear" as NA
d4 = d4 %>% replace_with_na_all( condition = ~.x == "Unclear" )
d4$x.mind.attr
# recode "Unclear" as NA
vars = c("x.mind.attr", "x.soc.norm", "x.id.victim", "x.impl", "x.pets")
setwd("~/Dropbox/Personal computer/Independent studies/2019/AWR (animal welfare review meat consumption)/Linked to OSF (AWR)/Data extraction/Dual review of intervention components")
d4 = read_xlsx("*component_coding_reconciled.xlsx")
# rename columns
d4 = rename(d4, x.mind.attr = "mind.attribution",
x.soc.norm = "social.norms",
x.id.victim = "id.victim",
x.impl = "impl.suggest",
x.pets = "pets")
# recode "Unclear" as NA
vars = c("x.mind.attr", "x.soc.norm", "x.id.victim", "x.impl", "x.pets")
table(d4$x.soc.norm, useNA = "ifany")
is.na(d4$authoryear)
sum(is.na(d4$authoryear))
setwd("~/Dropbox/Personal computer/Independent studies/2019/AWR (animal welfare review meat consumption)/Linked to OSF (AWR)/Data extraction/Dual review of intervention components")
d4 = read_xlsx("*component_coding_reconciled.xlsx")
d4 = d4 %>% filter( !is.na(authoryear) ) # remove spacer row
dim(d4)
expect_equal( nrow(d4), 100 )
# rename columns
d4 = rename(d4, x.mind.attr = "mind.attribution",
x.soc.norm = "social.norms",
x.id.victim = "id.victim",
x.impl = "impl.suggest",
x.pets = "pets")
sum(is.na(d4$authoryear))
table(d4$x.soc.norm, useNA = "ifany")
vars = c("x.mind.attr", "x.soc.norm", "x.id.victim", "x.impl", "x.pets")
d4 = d4 %>% replace_with_na_all( condition = ~.x == "Unclear" )
d4 = d4 %>% mutate_at( vars, as.numeric )
table(d4$x.soc.norm, useNA = "ifany")
View(d4)
# merge with main dataset
d = merge( d,
d4[ , c("unique", vars) ],
all.x = TRUE,
by = "unique" )
# should still have same size
expect_equal( nrow(d), 108 )
# save intermediate dataset
setwd(data.dir)
write.csv(d, "data_prepped_step3.csv", row.names = FALSE)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
#                          STEP 4 - CONVERT EFFECT SIZES TO RR SCALE                                      #
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
# read it back in
# check.names = FALSE retains spaces in variable names
setwd(data.dir)
d = read.csv("data_prepped_step3.csv", check.names = FALSE)
############################### CONVERT EFFECT SIZES - to RRs ###############################
# MM audited 2020-2-5
# types of effect measures
table( d$effect.measure )
# synchronize directions so that positive is always good
d$yi[ d$desired.direction == 1 ] = abs(d$yi[ d$desired.direction == 1 ])
d$yi[ d$desired.direction == 0 ] = -abs(d$yi[ d$desired.direction == 0 ])
# init estimates on scale used for analysis
d$logRR = NA
d$varlogRR = NA
##### RRs #####
# no conversion needed, obviously
d$logRR[ d$effect.measure == "log-rr" ] = d$yi[ d$effect.measure == "log-rr" ]
d$varlogRR[ d$effect.measure == "log-rr" ] = d$vi[ d$effect.measure == "log-rr" ]
##### SMDs #####
RR.stats = d_to_logRR( smd = d$yi[ d$effect.measure == "smd" ],
smd.se = sqrt(d$vi[ d$effect.measure == "smd" ]) )
d$logRR[ d$effect.measure == "smd" ] = RR.stats$logRR
d$varlogRR[ d$effect.measure == "smd" ] = RR.stats$varlogRR
# sanity check for absurd values
round( sort( exp(d$logRR) ), 2 )
sort(sqrt(d$varlogRR))
round( sort(d$logRR/sqrt(d$varlogRR)), 2 )  # z-scores
##### Calculate Approximate CI limits for Forest Plot #####
z.crit = qnorm(.975)
d$RR.lo = exp( d$logRR - z.crit * sqrt(d$varlogRR) )
d$RR.hi = exp( d$logRR + z.crit * sqrt(d$varlogRR) )
##### Affirmative vs. Nonaffirmative #####
d$pval = 2 * ( 1 - pnorm( abs(d$logRR) / sqrt(d$varlogRR) ) )
d$affirm = d$pval < 0.05 & d$logRR > 0
table(d$affirm)
# sanity check
# fine for RMDs to be NA since those are for the grams analysis
table( is.na(d$logRR), d$effect.measure)
# don't use these for the non-main-RR analyses
d$logRR[ d$use.rr.analysis == 0 ] = NA
d$varlogRR[ d$use.rr.analysis == 0 ] = NA
d$RR.lo[ d$use.rr.analysis == 0 ] = NA
d$RR.hi[ d$use.rr.analysis == 0 ] = NA
# save intermediate dataset
setwd(data.dir)
write.csv(d, "data_prepped_step4.csv", row.names = FALSE)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
#                                  STEP 5 - RECODE VARIABLES                                     #
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
# read it back in
# check.names = FALSE retains spaces in variable names
setwd(data.dir)
d = read.csv("data_prepped_step4.csv", check.names = FALSE)
############################### MAKE NEW VARIABLES AND RENAME THE EXISTING ONES ###############################
# MM audited 2020-2-5
d = d %>%
rename(
# meta-information about study and article
year = Year,
title = Title,
ref = `Ref #`,
journal = `Journal/conference (if peer-reviewed)`,
other.source = `Other source (if not peer-reviewed)`,
exclude.main = `Excluded challenge`,
borderline = `Borderline inclusion`,
mm.fave = `Among MM's favorites methodologically, exclusive of small sample size`,
stats.source = `Stats source (public data, data from author, paper, hopeless)`,
# study design characteristics
perc.male = `Percent male`,
avg.age = `Mean or median age`,
students = `Students (\"No\", \"General undergraduate\", \"Social sciences undergraduate\")`,
country = `Subject country`,
design = Design,
n.paper = `N (total analyzed sample size in paper, combining all substudies included here)`,
# intervention characteristics
x.has.text = `Intervention has text`,
x.suffer = `Intervention has specific description or images of animal suffering`,
x.has.visuals = `Intervention has visuals`,
x.pure.animals = `Intervention is purely animal welfare`,
x.suffer = `Intervention has specific description or images of animal suffering`,
x.rec = `Intervention recommendation (reduce, go vegan, go vegetarian, no request, other)`,
x.tailored = `Intervention personally tailored`,
x.min.exposed = `Total time exposed to intervention (minutes)`,
# subjective ones that were dual-coded:
x.mind.attr = mind.attribution,
x.soc.norm = social.norms,
x.id.victim = id.victim,
x.impl = impl.suggest,
x.pets = pets,
# outcome characteristics
y.cat = `Outcome category (purchase or consumption)`,
y.lag.days = `Outcome time lag from intervention (days)`,
y.other.eligible.foods = `Other food outcomes available?`,
# quality variables
qual.y.prox = `Outcome proximity (intended, self-reported, actual)`,
qual.missing = `Missing data (%)`,
qual.prereg = Preregistered,
qual.public.data = `Public data`,
qual.public.code = `Public code`,
# prose
prose.x = `Intervention (X)`,
prose.control = `Control condition`,
prose.outcome = `Outcome (Y)`,
prose.population = `Population`,
prose.y.other.eligible = `Additional eligible food outcomes`,
prose.notes = `Notes`
)
d = d %>%
rename(
# meta-information about study and article
year = Year,
title = Title,
ref = `Ref #`,
journal = `Journal/conference (if peer-reviewed)`,
other.source = `Other source (if not peer-reviewed)`,
exclude.main = `Excluded challenge`,
borderline = `Borderline inclusion`,
mm.fave = `Among MM's favorites methodologically, exclusive of small sample size`,
stats.source = `Stats source (public data, data from author, paper, hopeless)`,
# study design characteristics
perc.male = `Percent male`,
avg.age = `Mean or median age`,
students = `Students (\"No\", \"General undergraduate\", \"Social sciences undergraduate\")`,
country = `Subject country`,
design = Design,
n.paper = `N (total analyzed sample size in paper, combining all substudies included here)`,
# intervention characteristics
x.has.text = `Intervention has text`,
x.suffer = `Intervention has specific description or images of animal suffering`,
x.has.visuals = `Intervention has visuals`,
x.pure.animals = `Intervention is purely animal welfare`,
x.suffer = `Intervention has specific description or images of animal suffering`,
x.rec = `Intervention recommendation (reduce, go vegan, go vegetarian, no request, other)`,
x.tailored = `Intervention personally tailored`,
x.min.exposed = `Total time exposed to intervention (minutes)`,
# subjective ones that were dual-coded:
x.mind.attr = x.mind.attr,
x.soc.norm = x.soc.norm,
x.id.victim = x.id.victim,
x.impl = x.impl,
x.pets = x.pets,
# outcome characteristics
y.cat = `Outcome category (purchase or consumption)`,
y.lag.days = `Outcome time lag from intervention (days)`,
y.other.eligible.foods = `Other food outcomes available?`,
# quality variables
qual.y.prox = `Outcome proximity (intended, self-reported, actual)`,
qual.missing = `Missing data (%)`,
qual.prereg = Preregistered,
qual.public.data = `Public data`,
qual.public.code = `Public code`,
# prose
prose.x = `Intervention (X)`,
prose.control = `Control condition`,
prose.outcome = `Outcome (Y)`,
prose.population = `Population`,
prose.y.other.eligible = `Additional eligible food outcomes`,
prose.notes = `Notes`
)
# get rid of columns with capital letters (not used in analysis)
has.caps = tolower(names(d)) != names(d)
d = d[ , has.caps == FALSE | names(d) %in% c("logRR", "varlogRR", "RR.lo", "RR.hi") ]
############################### RECODE VARS AND MAKE NEW ONES ###############################
# MM audited 2020-2-5
##### Cast As Numeric #####
make.numeric = c("perc.male",
"x.min.exposed",
"qual.missing")
d = d %>%
mutate_at( make.numeric, as.numeric )
##### Simple Recodings #####
d$published = !is.na(d$journal)
d$y.lag.wks = d$y.lag.days/7
d$y.long.lag = d$y.lag.days >= 7
d$randomized = grepl(" RCT", d$design)  # the leading space prevents "NRCT" from counting as randomized
d$reproducible = (d$qual.prereg == "Yes") & (d$qual.public.data == "Yes")
d$x.long = d$x.min.exposed >= 5
# recode percent male as a 10-percentage point increase
d$perc.male.10 = d$perc.male/10
##### Recode Categorical Variables to Force Levels #####
library(car)
d$x.rec = recode_factor( d$x.rec,
"No request" = "a.No request",
"Reduce" = "b.Reduce",
"Go vegetarian" = "c.Go vegetarian",
"Go vegan" = "d.Go vegan",
"Mixed" = "e.Mixed")
d$qual.y.prox = dplyr::recode( d$qual.y.prox,
"Actual" = "a.Actual",
"Self-reported" = "b.Self-reported",
"Intended" = "c.Intended" )
# collapse categories
d$qual.y.prox2 = recode_factor( d$qual.y.prox,
"a.Actual" = "b.Actual or self-reported",
"b.Self-reported" = "b.Actual or self-reported",
"c.Intended" = "a.Intended")
# collapse categories
# any request vs. no request
d$x.makes.request = dplyr::recode( d$x.rec,
"a.No request" = 0,
"b.Reduce" = 1,
"c.Go vegetarian" = 1,
"d.Go vegan" = 1,
"e.Mixed" = 1)
d$design = recode_factor( d$design,
"Between-subjects RCT" = "a.Between-subjects RCT",
"Within-subject RCT" = "b.Within-subject RCT",
"Cluster RCT" = "c.Cluster RCT",
"Between-subjects NRCT" = "d.Between-subjects NRCT",
"Within-subject UCT" = "e.Within-subject UCT")
# rename in a risk-of-bias way
d$qual.sdb = dplyr::recode( d$qual.sdb,
"strong" = "a.Low",
"medium" = "b.Medium",
"weak" = "c.High",
"unclear" = "d.Unclear" )
d$qual.exch = dplyr::recode( d$qual.exch,
"strong" = "a.Low",
"medium" = "b.Medium",
"weak" = "c.High",
"unclear" = "d.Unclear" )
d$qual.gen = dplyr::recode( d$qual.gen,
"strong" = "a.Low",
"medium" = "b.Medium",
"weak" = "c.High",
"unclear" = "d.Unclear" )
# recode missing data
d[ d == "NR" ] = NA
d[ d == "na" ] = NA
############################### SIMPLIFIED "TABLE 1" DATASET ###############################
# MM audited 2020-2-5
d.small = d %>% dplyr::select( unique,
authoryear,
substudy,
title,
journal,
other.source,
borderline,
exclude.main,
prose.population,
country,
n.paper,
perc.male,
prose.x,
prose.control,
x.tailored,
x.has.text,
x.has.visuals,
x.suffer,
x.pure.animals,
x.mind.attr,
x.soc.norm,
x.id.victim,
x.impl,
x.pets, # bm
x.min.exposed,
x.rec,
prose.outcome,
y.cat,
y.other.eligible.foods,
prose.y.other.eligible,
design,
randomized,
qual.y.prox,
qual.missing,
qual.prereg,
qual.public.data,
qual.public.code,
qual.exch,
qual.gen,
qual.sdb,
effect.measure,
desired.direction,
yi,
vi,
logRR,
varlogRR,
RR.lo,
RR.hi )
############################### WRITE PREPPED DATA ###############################
setwd(data.dir)
# analysis dataset
write.csv(d, "prepped_data.csv", row.names = FALSE)
# same as above, but leaner and meaner choice of variables
write.csv(d.small, "prepped_data_lean.csv", row.names = FALSE)
names(d)
table(d$x.min.exposed)
table(d$x.mind.attr)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
#                                           PRELIMINARIES                                             #
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
original.data.dir = "~/Dropbox/Personal computer/Independent studies/2019/AWR (animal welfare review meat consumption)/Literature search/Full texts for review/*INCLUDED STUDIES"
code.dir = "~/Dropbox/Personal computer/Independent studies/2019/AWR (animal welfare review meat consumption)/Linked to OSF (AWR)/Data extraction/awr_data_extraction_git"
# helper code
setwd(code.dir)
source("helper_extraction.R")
# load packages
# this will reinstall the versions of all packages as they existed on
#  the date MBM analyzed
# https://cran.r-project.org/web/packages/checkpoint/vignettes/checkpoint.html
library(checkpoint)
checkpoint("2020-02-12")
library(dplyr)
library(mediation)
library(foreign)
library(sandwich)
library(metafor)
library(readxl)
library(tidyverse)
library(sjstats)
# library(grateful)
# cite_packages()
# MM audited 2020-2-2
setwd(original.data.dir)
setwd("Amiot 2018, #243")
dat = read.spss("data.sav", to.data.frame=TRUE)
library(foreign)
original.data.dir = "~/Dropbox/Personal computer/Independent studies/2019/AWR (animal welfare review meat consumption)/Literature search/Full texts for review/*INCLUDED STUDIES"
code.dir = "~/Dropbox/Personal computer/Independent studies/2019/AWR (animal welfare review meat consumption)/Linked to OSF (AWR)/Data extraction/awr_data_extraction_git"
# helper code
setwd(code.dir)
source("helper_extraction.R")
# must be done before checkpoint to avoid errors
library(foreign)
# load packages
# this will reinstall the versions of all packages as they existed on
#  the date MBM analyzed
# https://cran.r-project.org/web/packages/checkpoint/vignettes/checkpoint.html
library(checkpoint)
checkpoint("2020-02-12")
library(dplyr)
library(mediation)
library(sandwich)
library(metafor)
library(readxl)
library(tidyverse)
library(sjstats)
