authoryear = "ACE 2013b",
desired.direction = 0,
effect.measure = "smd",
interpretation = "Low vs. high animal product consumption",
use.rr.analysis = 1,
use.grams.analysis = 0,
use.veg.analysis = 0,
yi = -0.08581416,
vi = 0.01900103 )
##### Feltz 2019, #3858 #####
# MM audited 2020-2-5
# stats and sample sizes on page 11
# this is the SMD of the gain scores themselves
d = escalc_add_row( authoryear = "Feltz 2019",
substudy = NA,
desired.direction = 0,
effect.measure = "smd",
interpretation = "Within-subject change in animal product consumption",
use.rr.analysis = 1,
use.grams.analysis = 0,
use.veg.analysis = 0,
measure = "SMD",
m1i = 0.14,  # intervention
sd1i = 1.82,
n1i = 58,
m2i = -0.04,  # control
sd2i = 1.44,
n2i = 43 )
##### **Lackner 2019, #3858 #####
# MM audited 2020-2-5
d = dplyr::add_row(.data = d,
authoryear = "Lackner 2019",
desired.direction = 0,
effect.measure = "log-rr",
interpretation = "Low vs. high intended meat consumption",
use.rr.analysis = 1,
use.grams.analysis = 0,
use.veg.analysis = 0,
yi = -0.3542,
vi = 0.0817 )
###### **Norris 2014 #####
# MM audited 2020-2-5
# "Your Choice"
d = dplyr::add_row(.data = d,
authoryear = "Norris 2014",
substudy = '"Your Choice"',
desired.direction = 0,
effect.measure = "log-rr",
interpretation = "Low vs. high animal product consumption",
use.rr.analysis = 1,
use.grams.analysis = 0,
use.veg.analysis = 0,
yi = -0.108307,
vi = 0.01944182 )
# "Even If You Like Meat"
d = dplyr::add_row(.data = d,
authoryear = "Norris 2014",
substudy = '"Even If You Like Meat"',
desired.direction = 0,
effect.measure = "log-rr",
interpretation = "Low vs. high animal product consumption",
use.rr.analysis = 1,
use.grams.analysis = 0,
use.veg.analysis = 0,
yi = -0.05842669,
vi = 0.02098244 )
# ##### deLanauze 2019, #4033 #####
#
# # get SDs from p-values and means
#
# p =
#
# sd.cntrl = ( hw.cntrl / qt(.975, df = n.cntrl-1) ) * sqrt(n.cntrl)
# # sanity check
#
# d = dplyr::add_row(.data = d,
#                    authoryear = "deLanauze 2019",
#                    desired.direction = ,
#                    effect.measure = "smd",
#                    interpretation = "",
#                    use.rr.analysis = 1,
#                    use.grams.analysis = 0,
#                    use.veg.analysis = 0,
#                    yi = ,
#                    vi = )
#
#
# # confirm equivalence of transformation
# n0 = 100
# n1 = 100
# y0 = rnorm(n=n0, mean=0, sd=2)
# y1 = rnorm(n=n1, mean=y0 + 1, sd=2)  # make them correlated
# diff = y1-y0
#
# # independent-samples t-test
# t1 = t.test(y0, y1)$statistic
#
# # paired t-test
# t2 = t.test(y0, y1, paired = TRUE)$statistic
#
#
# # directly calculate d
# escalc( measure = "SMD",
#
#                 m1i = mean(y0),
#                 sd1i = sd(y0),
#                 n1i = n0,  # sample sizes on pg 52
#
#                 m2i = mean(y1),
#                 sd2i = sd(y1),
#                 n2i = n1 )
#
# t * sqrt( (n0+n1) / (n0*n1) )
#
# # they did a paired t-test, but we're treating as independent, so t-value we use
#  will be larger than it should be, so resulting d will be larger than what we
#  would have gotten with direct independent calculation
# save intermediate dataset
setwd(data.dir)
write.csv(d, "data_prepped_step1.csv", row.names = FALSE)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
#                                     STEP 2 - HIGH-BIAS CHALLENGES                                      #
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
# read it back in
setwd(data.dir)
d = read.csv("data_prepped_step1.csv")
##### #3865 30-day VeggieChallenge 2016 (ProVeg) #####
# Mensink Table 2a
escalc_add_row( authoryear = "30-Day VeggieChallenge (ProVeg) 2016",
substudy = NA,
desired.direction = 1,
effect.measure = "log-rr",
interpretation = "Being vegan after vs. before intervention",
use.rr.analysis = 1,
use.grams.analysis = 0,
use.veg.analysis = 0,
measure = "RR",
ai = 14, # pre-intervention vegans
bi = 60+20+102+32+60+15+17,  # pre-intervention non-vegans (2016)
ci = 36,  # post- vegans
di = 17+16+72+54+57+48+21 ) # post-non-vegans (2016)
14/(14+60+20+102+32+60+15+17)
36/(17+16+72+54+57+48+21)
##### #3865 30-day VeggieChallenge 2015 (ProVeg) #####
# MM audited 2020-2-5
# Mensink Table 2b
escalc_add_row( authoryear = "30-Day VeggieChallenge (ProVeg) 2015",
substudy = NA,
desired.direction = 1,
effect.measure = "log-rr",
interpretation = "Being vegan after vs. before intervention",
use.rr.analysis = 1,
use.grams.analysis = 0,
use.veg.analysis = 0,
measure = "RR",
ai = 15, # pre-intervention vegans
bi = 21+40+27+48,  # pre-intervention non-vegans
ci = 17,  # post- vegans
di = 4+29+32+35 ) # post-non-vegans
# sanity check
15/(15+21+40+27+48)
17/(17+4+29+32+35)
##### #3827 30-day VeggieChallenge 2017 (ProVeg) #####
# MM audited 2020-2-5
# Moleman 2018's Table 6:
# pre-intervention: 4% vegans
# post-intervention: 20% vegans
# hence the absurdly large point estimate
escalc_add_row( authoryear = "30-Day VeggieChallenge (ProVeg) 2017",
substudy = NA,
desired.direction = 1,
effect.measure = "log-rr",
interpretation = "Self-describing as vegan after vs. before challenge",
use.rr.analysis = 1,
use.grams.analysis = 0,
use.veg.analysis = 0,
measure = "RR",
ai = 697, # pre-intervention vegans
bi = 4727+7502+2573,  # pre-intervention non-vegans
ci = 799+1155+530+697,  # post- vegans
di = 1239+2473+4689+217+1658+2043 ) # post-non-vegans
# sanity check
( (799+1155+530+697) / (799+1155+530+697+1239+2473+4689+217+1658+2043) ) / ( 697 / (697+4727+7502+2573) )
##### #3828 10 Weeks to Vegan (Vegan Outreach) #####
# MM audited 2020-2-5
# "Food Intake" table
escalc_add_row( authoryear = "10 Weeks to Vegan (Vegan Outreach) 2018",
substudy = NA,
desired.direction = 1,
effect.measure = "log-rr",
interpretation = "Being vegan after vs. before intervention",
use.rr.analysis = 1,
use.grams.analysis = 0,
use.veg.analysis = 0,
measure = "RR",
ai = 43, # pre-intervention vegans
bi = 190+76,  # pre-intervention non-vegans
ci = 77,  # post- vegans
di = 140+92 ) # post-non-vegans
# sanity check
43/(190+76+43)
77/(140+92+77)
###### #3839 - Great Vegan Challenge (Animal Aid) #####
# MM audited 2020-2-5
# from Grassian
# measure bar heights on page 10 ("Dietary Trends Over Time")
total.px = 850  # total vertical height representing 100%
bl.vegans.px = 31  # height of vegan part of the "0 months" bar
fu.vegans.px = 240  # height of the vegan part of the "6 months" bar
N = 470 # pg 6
( bl.prop.vegan = bl.vegans.px/total.px )
( fu.prop.vegan = fu.vegans.px/total.px )
escalc_add_row( authoryear = "The Great Vegan Challenge (Animal Aid) 2016",
substudy = NA,
desired.direction = 1,
effect.measure = "log-rr",
interpretation = "Being vegan after vs. before intervention",
use.rr.analysis = 1,
use.grams.analysis = 0,
use.veg.analysis = 0,
measure = "RR",
ai = bl.prop.vegan * N, # pre-intervention vegans
bi = (1-bl.prop.vegan) * N,  # pre-intervention non-vegans
ci = fu.prop.vegan * N,  # post- vegans
di = (1-fu.prop.vegan) * N ) # post-non-vegans
##### #3840 Veganuary (Veganuary) 2014 #####
# MM audited 2020-2-5
# % staying vegan, % reducing consumption of each category
# both broken down by baseline diet
# not perfect comparison due to reporting limitations,
#  but am comparing the proportion who said they were vegan before challenge
#  to proportion saying they ate only vegan food during the challenge
# no pages in this document, but PDF has relevant stats circled
escalc_add_row( authoryear = "Veganuary (Veganuary) 2014",
substudy = NA,
desired.direction = 1,
effect.measure = "log-rr",
interpretation = "Being vegan after vs. during intervention",
use.rr.analysis = 1,
use.grams.analysis = 0,
use.veg.analysis = 0,
measure = "RR",
ai = .149 * 3325, # pre-intervention vegans
bi = (1-.149) * 3325,  # pre-intervention non-vegans
ci = 330,  # post- vegans
di = 711-330 ) # post-non-vegans
##### #3847, #3849 - Veganuary (Veganuary) 2016 #####
# MM audited 2020-2-5
# #3849 is a Fauna report, so more info available
# Table on page 3
escalc_add_row( authoryear = "Veganuary (Veganuary) 2016",
substudy = NA,
desired.direction = 1,
effect.measure = "log-rr",
interpretation = "Being vegan after vs. before intervention",
use.rr.analysis = 1,
use.grams.analysis = 0,
use.veg.analysis = 0,
measure = "RR",
ai = .19 * 20597, # pre-intervention vegans
bi = (1-.19) * 20597,  # pre-intervention non-vegans
ci = .54 * 3369,  # post- vegans
di = (1-.54) * 3369 ) # post-non-vegans
###### **#3826 Challenge 22+ (Animals Now) #####
# MM audited 2020-2-5
d = dplyr::add_row(.data = d,
authoryear = "Challenge 22+ (Animals Now) 2018",
substudy = NA,
desired.direction = 1,
effect.measure = "log-rr",
interpretation = "Being vegan after vs. before intervention",
use.rr.analysis = 1,
use.grams.analysis = 0,
use.veg.analysis = 0,
yi = 0.0739,
vi = 0.0001 )
##### #3853, #3854 - Veganuary (Veganuary) 2015 #####
# MM audited 2020-2-5
# 49% stayed vegan after six months
##### #3846 - Veganuary (Veganuary) 2017 #####
# MM audited 2020-2-5
# 67% intending to stay vegan
##### #3845 - Veganuary (Veganuary) 2018 #####
# MM audited 2020-2-5
# 62% intend to stay vegan
##### #3841 - Veganuary (Veganuary) 2019 #####
# MM audited 2020-2-5
# 47% intending to stay vegan
##### #3833, #3835 - Summer Vegan Pledge (Animal Aid) 2018 #####
# MM audited 2020-2-5
# 53% said they have remained vegan
##### #3834 Summer Vegan Pledge (Animal Aid) 2019 #####
# MM audited 2020-2-5
# 57% said they would remain vegan
# save intermediate dataset
setwd(data.dir)
write.csv(d, "data_prepped_step2.csv", row.names = FALSE)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
#                                     STEP 3 - MERGE IN QUALITATIVE DATA AND PREP ANALYSIS VARIABLES                                      #
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
# MM audited 2020-2-5
# read it back in
setwd(data.dir)
d = read.csv("data_prepped_step2.csv")
# how many unique studies?
length(unique(d$authoryear))
# how many point estimates?
nrow(d)
# qualitative data entered into Excel
setwd(data.dir)
# NOTE: this step breaks if cell values are hyphenated!
d2 = read_xlsx("Extracted qualitative data.xlsx", na = "NR")
# remove missing rows, used for human-readability
d2 = d2 %>% filter(!is.na(`First author last name`))
# for some reason, reads in years in an absurd format (e.g., "2018.0" as a string)
library(tidyverse)
d2$Year = str_remove(d2$Year, "[.]0")
# clean up carriage returns
d2$`Subject country` = str_replace_all(d2$`Subject country`, "[\n]" , "")
##### unique merger variable
d$unique = NA
d$unique[ is.na(d$substudy) ] = as.character(d$authoryear[ is.na(d$substudy) ])
d$unique[ !is.na(d$substudy) ] = paste( d$authoryear[ !is.na(d$substudy) ],
d$substudy[ !is.na(d$substudy) ],
sep = " ")
d$unique
# make pretty for eventual forest plot
d2$unique = NA
d2$unique[ is.na(d2$`Substudy #`) ] = paste( d2$`First author last name`[ is.na(d2$`Substudy #`) ],
d2$Year[ is.na(d2$`Substudy #`) ],
sep = " ")
d2$unique[ !is.na(d2$`Substudy #`) ] = paste( d2$`First author last name`[ !is.na(d2$`Substudy #`) ],
d2$Year[ !is.na(d2$`Substudy #`) ],
d2$`Substudy #`[ !is.na(d2$`Substudy #`) ],
sep = " ")
d2$unique
# look for IDs from effect sizes that aren't in the qualitative data spreadsheet
#  (should be none)
d$unique[ !d$unique %in% d2$unique ]
# studies in qualitative data for which we haven't entered effect sizes
# should occur only for hopeless studies (i.e., impossible to get stats)
known.hopeless = c( "Dowsett 2018",
"Diaz 2019",
"de Lanauze 2019",
"Summer Vegan Pledge (Animal Aid) 2018",
"Summer Vegan Pledge (Animal Aid) 2019",
"Veganuary (Veganuary) 2017",
"Veganuary (Veganuary) 2018",
"Veganuary (Veganuary) 2019" )
expect_equal( sort(d2$unique[ !d2$unique %in% d$unique ]), sort(known.hopeless) )
# merge them
d = merge( d,
d2,
all.x = TRUE,
by.x = "unique",
by.y = "unique" )
############################### MERGE IN SUBJECTIVELY-RATED QUALITY DATA ###############################
# MM audited 2020-2-5
setwd(data.dir)
setwd("Dual review of quality")
d3 = read.csv("subjective_data_full_prepped.csv")
# check for studies lacking entries in subjective data
# should be only high-bias challenges
d$authoryear[ !d$authoryear %in% d3$authoryear ]
# merge with main dataset
d = merge( d,
d3[ , c("authoryear", "qual.exch", "qual.sdb", "qual.gen") ],
all.x = TRUE,
by = "authoryear" )
# save intermediate dataset
setwd(data.dir)
write.csv(d, "data_prepped_step3.csv", row.names = FALSE)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
#                          STEP 4 - CONVERT EFFECT SIZES TO RR SCALE                                      #
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
# read it back in
# check.names = FALSE retains spaces in variable names
setwd(data.dir)
d = read.csv("data_prepped_step3.csv", check.names = FALSE)
############################### CONVERT EFFECT SIZES - to RRs ###############################
# MM audited 2020-2-5
# types of effect measures
table( d$effect.measure )
# synchronize directions so that positive is always good
d$yi[ d$desired.direction == 1 ] = abs(d$yi[ d$desired.direction == 1 ])
d$yi[ d$desired.direction == 0 ] = -abs(d$yi[ d$desired.direction == 0 ])
# init estimates on scale used for analysis
d$logRR = NA
d$varlogRR = NA
##### RRs #####
# no conversion needed, obviously
d$logRR[ d$effect.measure == "log-rr" ] = d$yi[ d$effect.measure == "log-rr" ]
d$varlogRR[ d$effect.measure == "log-rr" ] = d$vi[ d$effect.measure == "log-rr" ]
##### SMDs #####
RR.stats = d_to_logRR( smd = d$yi[ d$effect.measure == "smd" ],
smd.se = sqrt(d$vi[ d$effect.measure == "smd" ]) )
d$logRR[ d$effect.measure == "smd" ] = RR.stats$logRR
d$varlogRR[ d$effect.measure == "smd" ] = RR.stats$varlogRR
# sanity check for absurd values
round( sort( exp(d$logRR) ), 2 )
sort(sqrt(d$varlogRR))
round( sort(d$logRR/sqrt(d$varlogRR)), 2 )  # z-scores
##### Calculate Approximate CI limits for Forest Plot #####
z.crit = qnorm(.975)
d$RR.lo = exp( d$logRR - z.crit * sqrt(d$varlogRR) )
d$RR.hi = exp( d$logRR + z.crit * sqrt(d$varlogRR) )
##### Affirmative vs. Nonaffirmative #####
d$pval = 2 * ( 1 - pnorm( abs(d$logRR) / sqrt(d$varlogRR) ) )
d$affirm = d$pval < 0.05 & d$logRR > 0
table(d$affirm)
# sanity check
# fine for RMDs to be NA since those are for the grams analysis
table( is.na(d$logRR), d$effect.measure)
# don't use these for the non-main-RR analyses
d$logRR[ d$use.rr.analysis == 0 ] = NA
d$varlogRR[ d$use.rr.analysis == 0 ] = NA
d$RR.lo[ d$use.rr.analysis == 0 ] = NA
d$RR.hi[ d$use.rr.analysis == 0 ] = NA
# save intermediate dataset
setwd(data.dir)
write.csv(d, "data_prepped_step4.csv", row.names = FALSE)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
#                                  STEP 5 - RECODE VARIABLES                                     #
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
# read it back in
# check.names = FALSE retains spaces in variable names
setwd(data.dir)
d = read.csv("data_prepped_step4.csv", check.names = FALSE)
############################### MAKE NEW VARIABLES AND RENAME THE EXISTING ONES ###############################
# MM audited 2020-2-5
d = d %>%
rename(
# meta-information about study and article
year = Year,
title = Title,
ref = `Ref #`,
journal = `Journal/conference (if peer-reviewed)`,
other.source = `Other source (if not peer-reviewed)`,
exclude.main = `Excluded challenge`,
borderline = `Borderline inclusion`,
mm.fave = `Among MM's favorites methodologically, exclusive of small sample size`,
stats.source = `Stats source (public data, data from author, paper, hopeless)`,
# study design, intervention, and outcome variables
perc.male = `Percent male`,
country = `Subject country`,
design = Design,
n.paper = `N (total analyzed sample size in paper, combining all substudies included here)`,
x.has.text = `Intervention has text`,
x.suffer = `Intervention has specific description or images of animal suffering`,
x.has.visuals = `Intervention has visuals`,
x.pure.animals = `Intervention is purely animal welfare`,
x.suffer = `Intervention has specific description or images of animal suffering`,
x.rec = `Intervention recommendation (reduce, go vegan, go vegetarian, no request, other)`,
x.tailored = `Intervention personally tailored`,
x.min.exposed = `Total time exposed to intervention (minutes)`,
y.cat = `Outcome category (purchase or consumption)`,
y.lag.days = `Outcome time lag from intervention (days)`,
# quality variables
qual.y.prox = `Outcome proximity (intended, self-reported, actual)`,
qual.missing = `Missing data (%)`,
qual.prereg = Preregistered,
qual.public.data = `Public data`,
qual.public.code = `Public code`,
# prose
prose.x = `Intervention (X)`,
prose.control = `Control condition`,
prose.outcome = `Outcome (Y)`,
prose.population = `Population`,
prose.notes = `Notes`
)
# get rid of columns with capital letters (not used in analysis)
has.caps = tolower(names(d)) != names(d)
d = d[ , has.caps == FALSE | names(d) %in% c("logRR", "varlogRR", "RR.lo", "RR.hi") ]
############################### RECODE VARS AND MAKE NEW ONES ###############################
# MM audited 2020-2-5
##### Cast As Numeric #####
make.numeric = c("perc.male",
"x.min.exposed",
"qual.missing")
d = d %>%
mutate_at( make.numeric, as.numeric )
##### Simple Recodings #####
d$published = !is.na(d$journal)
d$y.lag.wks = d$y.lag.days/7
d$y.long.lag = d$y.lag.days >= 7
d$randomized = grepl(" RCT", d$design)  # the leading space prevents "NRCT" from counting as randomized
d$reproducible = (d$qual.prereg == "Yes") & (d$qual.public.data == "Yes")
d$x.long = d$x.min.exposed >= 5
# recode percent male as a 10-percentage point increase
d$perc.male.10 = d$perc.male/10
##### Recode Categorical Variables to Force Levels #####
library(car)
d$x.rec = recode_factor( d$x.rec,
"No request" = "a.No request",
"Reduce" = "b.Reduce",
"Go vegetarian" = "c.Go vegetarian",
"Go vegan" = "d.Go vegan",
"Mixed" = "e.Mixed")
d$qual.y.prox = dplyr::recode( d$qual.y.prox,
"Actual" = "a.Actual",
"Self-reported" = "b.Self-reported",
"Intended" = "c.Intended" )
# collapse categories
d$qual.y.prox2 = recode_factor( d$qual.y.prox,
"Actual" = "b.Actual or self-reported",
"Self-reported" = "b.Actual or self-reported",
"Intended" = "a.Intended")
library(metafor)
library(dplyr)
library(testthat)
library(readxl)
library(stringr)
