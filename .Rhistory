names( diffs2 )[3:7]
names( diffs2 )[3:7] = rated.vars
View(diffs2)
View(dats[[1]])
setwd(data.dir)
list.files()
setwd("Dual review of intervention components")
list.files()
write.csv(diffs2, "interrater_discrepancies.csv")
write.csv(dats[[1]], "component_coding_dr_prepped.csv")
write.csv(dats[[2]], "component_coding_mm_prepped.csv")
library(metafor)
library(dplyr)
library(testthat)
library(readxl)
library(stringr)
data.dir = "~/Dropbox/Personal computer/Independent studies/2019/AWR (animal welfare review meat consumption)/Linked to OSF (AWR)/Data extraction"
code.dir = "~/Dropbox/Personal computer/Independent studies/2019/AWR (animal welfare review meat consumption)/Linked to OSF (AWR)/Data extraction/awr_data_extraction_git"
# location of original datasets and code for reproducible studies
original.data.dir = "~/Dropbox/Personal computer/Independent studies/2019/AWR (animal welfare review meat consumption)/Literature search/Full texts for review/*INCLUDED STUDIES"
setwd(code.dir); source("helper_extraction.R")
# should we re-run the lengthy script that enters each effect size?
reenter.effect.sizes = FALSE
d = as.data.frame( matrix( ncol = 10, nrow = 0 ) )
names(d) = c( "authoryear",
"substudy",
"desired.direction",
"effect.measure",
"interpretation",
"use.rr.analysis",
"use.grams.analysis",
"use.veg.analysis",
"yi",
"vi")
# cast data types in empty df to avoid add_row trouble
d$authoryear = as.character(d$authoryear)
d$substudy = as.character(d$substudy)
d$interpretation = as.character(d$interpretation)
d$effect.measure = as.character(d$effect.measure)
# MM audited 2020-2-5
# bm
# read it back in
setwd(data.dir)
d = read.csv("data_prepped_step2.csv")
expect_equal( nrow(d), 108 )  # includes SSWS
# how many unique studies?
length(unique(d$authoryear))
# how many point estimates?
nrow(d)
# qualitative data entered into Excel
setwd(data.dir)
# NOTE: this step breaks if cell values are hyphenated!
d2 = read_xlsx("Extracted qualitative data.xlsx", na = "NR")
# remove missing rows, used for human-readability
d2 = d2 %>% filter(!is.na(`First author last name`))
# for some reason, reads in years in an absurd format (e.g., "2018.0" as a string)
library(tidyverse)
d2$Year = str_remove(d2$Year, "[.]0")
# clean up carriage returns
d2$`Subject country` = str_replace_all(d2$`Subject country`, "[\n]" , "")
##### unique merger variable
d$unique = NA
d$unique[ is.na(d$substudy) ] = as.character(d$authoryear[ is.na(d$substudy) ])
d$unique[ !is.na(d$substudy) ] = paste( d$authoryear[ !is.na(d$substudy) ],
d$substudy[ !is.na(d$substudy) ],
sep = " ")
d$unique
# make pretty for eventual forest plot
d2$unique = NA
d2$unique[ is.na(d2$`Substudy #`) ] = paste( d2$`First author last name`[ is.na(d2$`Substudy #`) ],
d2$Year[ is.na(d2$`Substudy #`) ],
sep = " ")
d2$unique[ !is.na(d2$`Substudy #`) ] = paste( d2$`First author last name`[ !is.na(d2$`Substudy #`) ],
d2$Year[ !is.na(d2$`Substudy #`) ],
d2$`Substudy #`[ !is.na(d2$`Substudy #`) ],
sep = " ")
d2$unique
# look for IDs from effect sizes that aren't in the qualitative data spreadsheet
#  (should be none)
expect_equal( d$unique[ !d$unique %in% d2$unique ], character(0) )
# studies in qualitative data for which we haven't entered effect sizes
# should occur only for hopeless studies (i.e., impossible to get stats)
known.hopeless = c( "Dowsett 2018",
#"Diaz 2019",  # removed from spreadsheet because we couldn't determine its eligibility
"de Lanauze 2019",
"Summer Vegan Pledge (Animal Aid) 2018",
"Summer Vegan Pledge (Animal Aid) 2019",
"Veganuary (Veganuary) 2017",
"Veganuary (Veganuary) 2018",
"Veganuary (Veganuary) 2019" )
expect_equal( sort(d2$unique[ !d2$unique %in% d$unique ]), sort(known.hopeless) )
# merge them
d = merge( d,
d2,
all.x = TRUE,
by.x = "unique",
by.y = "unique" )
############################### MERGE IN SUBJECTIVELY-RATED QUALITY DATA ###############################
# MM audited 2020-2-5
# read in subjective quality data
setwd(data.dir)
setwd("Dual review of quality")
d3 = read.csv("subjective_data_full_prepped.csv")
# check for studies lacking entries in subjective data
# should be only high-bias challenges
d$authoryear[ !d$authoryear %in% d3$authoryear ]
# merge with main dataset
d = merge( d,
d3[ , c("authoryear", "qual.exch", "qual.sdb", "qual.gen") ],
all.x = TRUE,
by = "authoryear" )
############################### MERGE IN INTERVENTION COMPONENTS DATA ###############################
# ~~~ TEMP ONLY: using only MM's data until we've reconciled them all
setwd("~/Dropbox/Personal computer/Independent studies/2019/AWR (animal welfare review meat consumption)/Linked to OSF (AWR)/Data extraction/Dual review of intervention components")
d4 = read_xlsx("*component_coding_reconciled.xlsx")
d4 = d4 %>% filter( !is.na(authoryear) ) # remove spacer row
expect_equal( nrow(d4), 100 )
# rename columns
d4 = rename(d4, x.mind.attr = "mind.attribution",
x.soc.norm = "social.norms",
x.id.victim = "id.victim",
x.impl = "impl.suggest",
x.pets = "pets")
# recode "Unclear" as NA
vars = c("x.mind.attr", "x.soc.norm", "x.id.victim", "x.impl", "x.pets")
d4 = d4 %>% replace_with_na_all( condition = ~.x == "Unclear" )
d4 = d4 %>% mutate_at( vars, as.numeric )
# sanity check (compare to before recoding "Unclear")
# table(d4$x.soc.norm, useNA = "ifany")
# check for studies lacking entries in subjective data
# should be only SSWS studies
d$authoryear[ !d$authoryear %in% d4$authoryear ]
# merge with main dataset
d = merge( d,
d4[ , c("unique", vars) ],
all.x = TRUE,
by = "unique" )
# should still have same size
expect_equal( nrow(d), 108 )
# save intermediate dataset
setwd(data.dir)
write.csv(d, "data_prepped_step3.csv", row.names = FALSE)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
#                          STEP 4 - CONVERT EFFECT SIZES TO RR SCALE                                      #
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
# read it back in
# check.names = FALSE retains spaces in variable names
setwd(data.dir)
d = read.csv("data_prepped_step3.csv", check.names = FALSE)
############################### CONVERT EFFECT SIZES - to RRs ###############################
# MM audited 2020-2-5
# types of effect measures
table( d$effect.measure )
# synchronize directions so that positive is always good
d$yi[ d$desired.direction == 1 ] = abs(d$yi[ d$desired.direction == 1 ])
d$yi[ d$desired.direction == 0 ] = -abs(d$yi[ d$desired.direction == 0 ])
# init estimates on scale used for analysis
d$logRR = NA
d$varlogRR = NA
##### RRs #####
# no conversion needed, obviously
d$logRR[ d$effect.measure == "log-rr" ] = d$yi[ d$effect.measure == "log-rr" ]
d$varlogRR[ d$effect.measure == "log-rr" ] = d$vi[ d$effect.measure == "log-rr" ]
##### SMDs #####
RR.stats = d_to_logRR( smd = d$yi[ d$effect.measure == "smd" ],
smd.se = sqrt(d$vi[ d$effect.measure == "smd" ]) )
d$logRR[ d$effect.measure == "smd" ] = RR.stats$logRR
d$varlogRR[ d$effect.measure == "smd" ] = RR.stats$varlogRR
# sanity check for absurd values
round( sort( exp(d$logRR) ), 2 )
sort(sqrt(d$varlogRR))
round( sort(d$logRR/sqrt(d$varlogRR)), 2 )  # z-scores
##### Calculate Approximate CI limits for Forest Plot #####
z.crit = qnorm(.975)
d$RR.lo = exp( d$logRR - z.crit * sqrt(d$varlogRR) )
d$RR.hi = exp( d$logRR + z.crit * sqrt(d$varlogRR) )
##### Affirmative vs. Nonaffirmative #####
d$pval = 2 * ( 1 - pnorm( abs(d$logRR) / sqrt(d$varlogRR) ) )
d$affirm = d$pval < 0.05 & d$logRR > 0
table(d$affirm)
# sanity check
# fine for RMDs to be NA since those are for the grams analysis
table( is.na(d$logRR), d$effect.measure)
# don't use these for the non-main-RR analyses
d$logRR[ d$use.rr.analysis == 0 ] = NA
d$varlogRR[ d$use.rr.analysis == 0 ] = NA
d$RR.lo[ d$use.rr.analysis == 0 ] = NA
d$RR.hi[ d$use.rr.analysis == 0 ] = NA
# save intermediate dataset
setwd(data.dir)
write.csv(d, "data_prepped_step4.csv", row.names = FALSE)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
#                                  STEP 5 - RECODE VARIABLES                                     #
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
# read it back in
# check.names = FALSE retains spaces in variable names
setwd(data.dir)
d = read.csv("data_prepped_step4.csv", check.names = FALSE)
############################### MAKE NEW VARIABLES AND RENAME THE EXISTING ONES ###############################
# MM audited 2020-2-5
d = d %>%
rename(
# meta-information about study and article
year = Year,
title = Title,
ref = `Ref #`,
journal = `Journal/conference (if peer-reviewed)`,
other.source = `Other source (if not peer-reviewed)`,
exclude.main = `Excluded challenge`,
borderline = `Borderline inclusion`,
mm.fave = `Among MM's favorites methodologically, exclusive of small sample size`,
stats.source = `Stats source (public data, data from author, paper, hopeless)`,
# study design characteristics
perc.male = `Percent male`,
age = `Mean or median age`,
students = `Students ("No", "General undergraduates", "Social sciences undergraduates", "Mixed")`,
country = `Subject country`,
design = Design,
n.paper = `N (total analyzed sample size in paper, combining all substudies included here)`,
# intervention characteristics
x.has.text = `Intervention has text`,
x.suffer = `Intervention has specific description or images of animal suffering`,
x.has.visuals = `Intervention has visuals`,
x.pure.animals = `Intervention is purely animal welfare`,
x.suffer = `Intervention has specific description or images of animal suffering`,
x.rec = `Intervention recommendation (reduce, go vegan, go vegetarian, no request, other)`,
x.tailored = `Intervention personally tailored`,
x.min.exposed = `Total time exposed to intervention (minutes)`,
# subjective ones that were dual-coded:
x.mind.attr = x.mind.attr,
x.soc.norm = x.soc.norm,
x.id.victim = x.id.victim,
x.impl = x.impl,
x.pets = x.pets,
# outcome characteristics
y.cat = `Outcome category (purchase or consumption)`,
y.lag.days = `Outcome time lag from intervention (days)`,
y.other.eligible.foods = `Other food outcomes available?`,
# quality variables
qual.y.prox = `Outcome proximity (intended, self-reported, actual)`,
qual.missing = `Missing data (%)`,
qual.prereg = Preregistered,
qual.public.data = `Public data`,
qual.public.code = `Public code`,
# prose
prose.x = `Intervention (X)`,
prose.control = `Control condition`,
prose.outcome = `Outcome (Y)`,
prose.population = `Population`,
prose.y.other.eligible = `Additional eligible food outcomes`,
prose.notes = `Notes`
)
# get rid of columns with capital letters (not used in analysis)
has.caps = tolower(names(d)) != names(d)
d = d[ , has.caps == FALSE | names(d) %in% c("logRR", "varlogRR", "RR.lo", "RR.hi") ]
############################### RECODE VARS AND MAKE NEW ONES ###############################
# MM audited 2020-2-5
##### Cast As Numeric #####
make.numeric = c("perc.male",
"x.min.exposed",
"qual.missing")
d = d %>%
mutate_at( make.numeric, as.numeric )
##### Simple Recodings #####
d$published = !is.na(d$journal)
d$y.lag.wks = d$y.lag.days/7
d$y.long.lag = d$y.lag.days >= 7
d$randomized = grepl(" RCT", d$design)  # the leading space prevents "NRCT" from counting as randomized
d$reproducible = (d$qual.prereg == "Yes") & (d$qual.public.data == "Yes")
d$x.long = d$x.min.exposed >= 5
# recode percent male as a 10-percentage point increase
d$perc.male.10 = d$perc.male/10
# recode age as a 5-year increase
d$age.5 = d$age/5
##### Recode Categorical Variables to Force Levels #####
library(car)
d$x.rec = recode_factor( d$x.rec,
"No request" = "a.No request",
"Reduce" = "b.Reduce",
"Go vegetarian" = "c.Go vegetarian",
"Go vegan" = "d.Go vegan",
"Mixed" = "e.Mixed")
d$qual.y.prox = dplyr::recode( d$qual.y.prox,
"Actual" = "a.Actual",
"Self-reported" = "b.Self-reported",
"Intended" = "c.Intended" )
# collapse categories
d$qual.y.prox2 = recode_factor( d$qual.y.prox,
"a.Actual" = "b.Actual or self-reported",
"b.Self-reported" = "b.Actual or self-reported",
"c.Intended" = "a.Intended")
# collapse categories
# any request vs. no request
d$x.makes.request = dplyr::recode( d$x.rec,
"a.No request" = 0,
"b.Reduce" = 1,
"c.Go vegetarian" = 1,
"d.Go vegan" = 1,
"e.Mixed" = 1)
d$design = recode_factor( d$design,
"Between-subjects RCT" = "a.Between-subjects RCT",
"Within-subject RCT" = "b.Within-subject RCT",
"Cluster RCT" = "c.Cluster RCT",
"Between-subjects NRCT" = "d.Between-subjects NRCT",
"Within-subject UCT" = "e.Within-subject UCT")
# rename in a risk-of-bias way
d$qual.sdb = dplyr::recode( d$qual.sdb,
"strong" = "a.Low",
"medium" = "b.Medium",
"weak" = "c.High",
"unclear" = "d.Unclear" )
d$qual.exch = dplyr::recode( d$qual.exch,
"strong" = "a.Low",
"medium" = "b.Medium",
"weak" = "c.High",
"unclear" = "d.Unclear" )
d$qual.gen = dplyr::recode( d$qual.gen,
"strong" = "a.Low",
"medium" = "b.Medium",
"weak" = "c.High",
"unclear" = "d.Unclear" )
# recode missing data
d[ d == "NR" ] = NA
d[ d == "na" ] = NA
############################### SIMPLIFIED "TABLE 1" DATASET ###############################
# MM audited 2020-2-5
d.small = d %>% dplyr::select( unique,
authoryear,
substudy,
title,
journal,
other.source,
borderline,
exclude.main,
prose.population,
country,
n.paper,
perc.male,
prose.x,
prose.control,
x.tailored,
x.has.text,
x.has.visuals,
x.suffer,
x.pure.animals,
x.mind.attr,
x.soc.norm,
x.id.victim,
x.impl,
x.pets, # bm
x.min.exposed,
x.rec,
prose.outcome,
y.cat,
y.other.eligible.foods,
prose.y.other.eligible,
design,
randomized,
qual.y.prox,
qual.missing,
qual.prereg,
qual.public.data,
qual.public.code,
qual.exch,
qual.gen,
qual.sdb,
effect.measure,
desired.direction,
yi,
vi,
logRR,
varlogRR,
RR.lo,
RR.hi )
############################### WRITE PREPPED DATA ###############################
setwd(data.dir)
# analysis dataset
write.csv(d, "prepped_data.csv", row.names = FALSE)
# same as above, but leaner and meaner choice of variables
write.csv(d.small, "prepped_data_lean.csv", row.names = FALSE)
dim(d.small)
table(d$students)
data.dir = "~/Dropbox/Personal computer/Independent studies/2019/AWR (animal welfare review meat consumption)/Linked to OSF (AWR)/Data extraction"
library(dplyr)
library(tidyverse)
library(naniar)
library(tidyr)
library(readxl)
library(irr)
library(testthat)
setwd(data.dir)
setwd("Dual review of intervention components")
# read in each coder's dataset
dd = read.csv("component_coding_dr.csv")  # DR
dm = read_xlsx("component_coding_mm.xlsx")  # MBM
dats = list(dd, dm)
head(dd)
head(dm)
# remove blank spacer rows
dats = lapply( dats, FUN = function(.d) .d = .d[ !is.na(.d$authoryear), ] )
expect_equal( unlist( lapply(dats, nrow) ), c(100, 100) )
# recode missing data in all columns
dats = lapply( dats, FUN = function(.d) .d %>% replace_with_na_all( condition = ~.x == "Unclear" ) )
head(dm)
head(dats[[2]])
rated.vars = c("mind.attribution", "social.norms", "id.victim", "impl.suggest", "pets")
lapply( dats, FUN = function(.d) apply(.d[,rated.vars], 2, table) )
# cast the 0/1s as numeric
dats = lapply( dats, FUN = function(.d) .d %>% mutate_at( rated.vars, as.numeric ) )
lapply( dats, FUN = function(.d) apply(.d[,rated.vars], 2, table) )
x1 = dats[[1]][,rated.vars]
x2 = dats[[2]][,rated.vars]
headd(x1)
head(x1)
cardio = sum(c(80,6060,10,7.5,12.5,60))
cardio/60
cardio = sum(c(80,60,60,10,7.5,12.5,60))
cardio/60
lift = sum(c(46,3,45,53,3,0))
lift = sum(c(46,3,45,53,3,20))
lift/60
10/.13
80/.13
80/8.5
80/8.25
10*8.5
45*8.33
(45*8.33)/60
(50*8.33)/60
(45*8)/60
1350-278.57
# MM audited 2020-2-2
setwd(original.data.dir)
setwd("Anderson 2016, #3742")
# only Study 3 is eligible
dat = read.spss("Study3_data.sav", to.data.frame=TRUE)
# remove subjects who didn't follow instructions, as in article
dat = dat[ dat$ReadDescFilter == "read desc", ]
# sample size vs. expected
expect_equal( nrow(dat), 114 )
# missing data: 1/114
# but note that per the article, actually original sample was 117 but they had
#  made exclusions before giving us this dataset
# "CT" = control; "FF" = factory farming
table(is.na(dat$CTeaten) | is.na(dat$FFeaten))
dat = dat[ !is.na(dat$CTeaten) & !is.na(dat$FFeaten), ]
original.data.dir = "~/Dropbox/Personal computer/Independent studies/2019/AWR (animal welfare review meat consumption)/Literature search/Full texts for review/*INCLUDED STUDIES"
code.dir = "~/Dropbox/Personal computer/Independent studies/2019/AWR (animal welfare review meat consumption)/Linked to OSF (AWR)/Data extraction/awr_data_extraction_git"
# helper code
setwd(code.dir)
source("helper_extraction.R")
# must be done before checkpoint to avoid errors
library(foreign)
library(metafor)
library(dplyr)
# load packages
# this will reinstall the versions of all packages as they existed on
#  the date MBM analyzed
# https://cran.r-project.org/web/packages/checkpoint/vignettes/checkpoint.html
library(checkpoint)
checkpoint("2020-02-12")
library(mediation)
library(sandwich)
library(metafor)
library(readxl)
library(tidyverse)
library(sjstats)
library(data.table)
# library(grateful)
# cite_packages()
setwd(original.data.dir)
setwd("Anderson 2016, #3742")
# only Study 3 is eligible
dat = read.spss("Study3_data.sav", to.data.frame=TRUE)
# remove subjects who didn't follow instructions, as in article
dat = dat[ dat$ReadDescFilter == "read desc", ]
# sample size vs. expected
expect_equal( nrow(dat), 114 )
library(testthat)
# MM audited 2020-2-2
setwd(original.data.dir)
setwd("Anderson 2016, #3742")
# only Study 3 is eligible
dat = read.spss("Study3_data.sav", to.data.frame=TRUE)
# remove subjects who didn't follow instructions, as in article
dat = dat[ dat$ReadDescFilter == "read desc", ]
# sample size vs. expected
expect_equal( nrow(dat), 114 )
# missing data: 1/114
# but note that per the article, actually original sample was 117 but they had
#  made exclusions before giving us this dataset
# "CT" = control; "FF" = factory farming
table(is.na(dat$CTeaten) | is.na(dat$FFeaten))
dat = dat[ !is.na(dat$CTeaten) & !is.na(dat$FFeaten), ]
# sex and age
prop.table(table(dat$Gender))
mean(dat$Age)
# sanity check (compare visually to Fig 5)
mean(dat$CTeaten)
mean(dat$FFeaten)
# missing data: 1/114
# but note that per the article, actually original sample was 117 but they had
#  made exclusions before giving us this dataset
# "CT" = control; "FF" = factory farming
table(is.na(dat$CTeaten) | is.na(dat$FFeaten))
dat = dat[ !is.na(dat$CTeaten) & !is.na(dat$FFeaten), ]
1/114
mean(c(2,4.5,5,4,2,5,4))
mean(c(2,5,1.5,4,4,4,4))
mean(c(1.5,4,1.5,3,5,1.5,3))
.14*sal
.14*sal
